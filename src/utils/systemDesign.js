export const systemDesign = [
    {
        "id": 1,
        "topic": "system-design",
        "question": "What is difference between API Gateway and Load Balancer?",
        "answer": "API Gateways and Load Balancers are both critical components in modern application architectures, but they serve different purposes and operate at different layers of the network. Here's a breakdown of their differences:\r\n\r\n       API Gateway\r\n1.   Purpose  :\r\n   - Primarily used to manage, route, and secure API requests between clients and services.\r\n   - Handles tasks such as request routing, composition, protocol translation, rate limiting, and security enforcement.\r\n\r\n2.   Functions  :\r\n   -   Request Routing  : Directs API calls to appropriate backend services.\r\n   -   Authentication and Authorization  : Enforces security policies, verifies API keys, tokens, etc.\r\n   -   Rate Limiting and Throttling  : Controls the rate at which consumers can call the APIs.\r\n   -   Protocol Translation  : Translates between different protocols (e.g., HTTP to WebSocket).\r\n   -   Caching  : Stores responses to reduce backend load.\r\n   -   Analytics and Monitoring  : Provides metrics and logs about API usage.\r\n\r\n3.   Layer  :\r\n   - Operates at the application layer (Layer 7 of the OSI model).\r\n\r\n4.   Typical Use Cases  :\r\n   - Microservices architectures where services need to be exposed securely to clients.\r\n   - Managing internal and external APIs with additional security, monitoring, and rate limiting.\r\n\r\n       Load Balancer\r\n1.   Purpose  :\r\n   - Distributes incoming network or application traffic across multiple servers to ensure no single server becomes overloaded.\r\n   - Enhances application availability and reliability by distributing traffic.\r\n\r\n2.   Functions  :\r\n   -   Traffic Distribution  : Balances incoming requests across multiple servers to ensure even load distribution.\r\n   -   Health Checks  : Monitors the health of servers and reroutes traffic away from unhealthy or failed instances.\r\n   -   Session Persistence  : Ensures a user is directed to the same server during a session.\r\n   -   SSL Termination  : Decrypts incoming SSL traffic to reduce the load on backend servers.\r\n\r\n3.   Layer  :\r\n   - Can operate at multiple layers:\r\n     -   Layer 4 (Transport Layer)  : Distributes traffic based on IP address and TCP/UDP ports.\r\n     -   Layer 7 (Application Layer)  : Distributes traffic based on HTTP headers, cookies, and URLs.\r\n\r\n4.   Typical Use Cases  :\r\n   - Scalable web applications requiring high availability.\r\n   - Distributing traffic across multiple servers or instances to avoid overloading.\r\n\r\n       Key Differences\r\n-   Scope  : API Gateways focus on managing APIs and their traffic, offering advanced features like security, monitoring, and transformation. Load Balancers focus on evenly distributing network traffic across servers to ensure high availability and reliability.\r\n-   Layer  : API Gateways work at the application layer (Layer 7), while Load Balancers can work at both the transport layer (Layer 4) and the application layer (Layer 7).\r\n-   Functionality  : API Gateways provide functionalities specifically for API management, such as authentication, rate limiting, and protocol translation. Load Balancers are concerned with distributing traffic to prevent server overload and ensure availability.\r\n\r\nIn summary, while both API Gateways and Load Balancers can handle some overlapping tasks like routing and health checks, their primary functions and use cases are distinct. An API Gateway is tailored for managing API traffic with additional features for security and monitoring, whereas a Load Balancer is designed to distribute network traffic to maintain application performance and reliability.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 2,
        "topic": "system-design",
        "question": "What is difference between Reverse Proxy and Forward Proxy?",
        "answer": "Reverse proxies and forward proxies both act as intermediaries between clients and servers, but they serve different purposes and operate in different contexts. Hereâ€™s a detailed comparison:\r\n\r\n       Forward Proxy\r\n1.   Purpose  :\r\n   - A forward proxy acts on behalf of clients to access resources on the internet. It forwards client requests to the internet and returns the responses back to the clients.\r\n   \r\n2.   Use Case  :\r\n   -   Client-side Caching  : Speeds up access to frequently visited sites.\r\n   -   Anonymity and Privacy  : Hides the client's IP address from the server.\r\n   -   Content Filtering  : Blocks access to certain websites (e.g., in a corporate or educational environment).\r\n   -   Bypassing Restrictions  : Accesses blocked or geo-restricted content.\r\n\r\n3.   Operation  :\r\n   - Clients know they are using a proxy and must configure their devices to use the forward proxy.\r\n   - The proxy server makes requests to external servers on behalf of the client.\r\n\r\n4.   Example  :\r\n   - A company proxy server that employees must configure in their browser settings to access the internet.\r\n\r\n       Reverse Proxy\r\n1.   Purpose  :\r\n   - A reverse proxy acts on behalf of servers to handle requests from clients. It forwards client requests to appropriate backend servers and returns the responses back to the clients.\r\n   \r\n2.   Use Case  :\r\n   -   Load Balancing  : Distributes client requests across multiple servers to ensure no single server is overloaded.\r\n   -   Caching  : Reduces load on backend servers by caching responses.\r\n   -   SSL Termination  : Handles SSL encryption/decryption to offload this task from backend servers.\r\n   -   Security  : Hides the details of backend servers, adds a layer of security, and can perform filtering and authentication.\r\n\r\n3.   Operation  :\r\n   - Clients are typically unaware they are communicating with a reverse proxy; they interact with it as if it is the server.\r\n   - The reverse proxy routes the requests to the appropriate backend servers based on various criteria (e.g., load balancing algorithms).\r\n\r\n4.   Example  :\r\n   - A web application uses a reverse proxy to distribute incoming client requests to multiple web servers.\r\n\r\n       Key Differences\r\n-   Direction of Proxying  :\r\n  -   Forward Proxy  : Clients connect to the proxy, which then connects to external servers.\r\n  -   Reverse Proxy  : Clients connect to the proxy, which then connects to internal servers.\r\n\r\n-   Client Awareness  :\r\n  -   Forward Proxy  : Clients need to know and configure the proxy settings.\r\n  -   Reverse Proxy  : Clients are generally unaware of the proxy.\r\n\r\n-   Primary Users  :\r\n  -   Forward Proxy  : Used by clients or end-users to access external resources.\r\n  -   Reverse Proxy  : Used by servers to manage incoming client requests.\r\n\r\n-   Common Functions  :\r\n  -   Forward Proxy  : Anonymity, content filtering, and bypassing restrictions.\r\n  -   Reverse Proxy  : Load balancing, SSL termination, caching, and security.\r\n\r\n       Summary\r\nIn summary, a forward proxy is primarily used by clients to facilitate access to external resources, providing benefits like anonymity and content filtering. A reverse proxy, on the other hand, is used by servers to manage client requests, providing benefits such as load balancing, improved security, and SSL termination.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 3,
        "topic": "system-design",
        "question": "What is difference between Horizontal scaling and vertical scaling?",
        "answer": "Horizontal scaling and vertical scaling are two methods of increasing the capacity and performance of a system to handle increased load. Here are the key differences between the two:\r\n\r\n       Horizontal Scaling (Scaling Out)\r\n1.   Concept  :\r\n   - Horizontal scaling involves adding more machines or instances to a system. This means increasing the number of servers in your environment.\r\n\r\n2.   Implementation  :\r\n   - Distribute the load across multiple servers or nodes.\r\n   - Often involves load balancers to distribute traffic evenly across servers.\r\n   - Requires data replication and synchronization across nodes.\r\n\r\n3.   Advantages  :\r\n   -   Fault Tolerance  : Improved fault tolerance and high availability since the load is distributed across multiple servers.\r\n   -   Scalability  : Easier to scale out by adding more servers as needed.\r\n   -   Flexibility  : Can scale out using commodity hardware.\r\n   -   Performance  : Potential for significant performance improvements by handling more simultaneous requests.\r\n\r\n4.   Challenges  :\r\n   -   Complexity  : More complex to manage due to the need for load balancing, data replication, and distributed computing.\r\n   -   Consistency  : Ensuring data consistency across multiple nodes can be challenging.\r\n   -   Networking  : Increased network traffic and latency due to inter-node communication.\r\n\r\n5.   Typical Use Cases  :\r\n   - Web applications with high traffic.\r\n   - Distributed databases and microservices architectures.\r\n\r\n       Vertical Scaling (Scaling Up)\r\n1.   Concept  :\r\n   - Vertical scaling involves adding more resources (CPU, RAM, storage) to an existing machine. This means increasing the capacity of a single server.\r\n\r\n2.   Implementation  :\r\n   - Upgrade the hardware of the existing server (e.g., adding more RAM, faster CPUs, more storage).\r\n   - Often requires downtime or reconfiguration to take advantage of new resources.\r\n\r\n3.   Advantages  :\r\n   -   Simplicity  : Easier to implement and manage since it involves a single server.\r\n   -   Consistency  : No need for data replication or synchronization since everything is on one server.\r\n   -   Performance  : Can lead to significant performance improvements for applications that are limited by CPU or memory.\r\n\r\n4.   Challenges  :\r\n   -   Scalability Limits  : Limited by the maximum capacity of the server hardware. Thereâ€™s a ceiling to how much you can scale up.\r\n   -   Single Point of Failure  : If the server fails, the entire system can go down.\r\n   -   Cost  : Upgrading hardware can be expensive, especially for high-end components.\r\n\r\n5.   Typical Use Cases  :\r\n   - Applications with moderate growth expectations.\r\n   - Systems with predictable and stable workloads.\r\n   - Initial stages of an application where demand is not yet high.\r\n\r\n       Key Differences\r\n-   Approach  :\r\n  -   Horizontal Scaling  : Adds more servers to the system.\r\n  -   Vertical Scaling  : Adds more resources to an existing server.\r\n\r\n-   Scalability  :\r\n  -   Horizontal Scaling  : Potentially unlimited scalability by adding more servers.\r\n  -   Vertical Scaling  : Limited by the capacity of a single server.\r\n\r\n-   Complexity  :\r\n  -   Horizontal Scaling  : More complex due to the need for load balancing, data replication, and synchronization.\r\n  -   Vertical Scaling  : Simpler as it involves only one server.\r\n\r\n-   Fault Tolerance  :\r\n  -   Horizontal Scaling  : Better fault tolerance as the load is distributed across multiple servers.\r\n  -   Vertical Scaling  : Single point of failure as all resources are on one server.\r\n\r\n       Summary\r\nIn summary, horizontal scaling (scaling out) involves adding more servers to distribute the load, which enhances fault tolerance and scalability but increases complexity. Vertical scaling (scaling up) involves upgrading the resources of a single server, which is simpler to manage but has scalability limits and presents a single point of failure. The choice between the two depends on the specific requirements and constraints of the system being designed or maintained.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 4,
        "topic": "system-design",
        "question": "What is difference between Microservices and Monolithic architecture?",
        "answer": "Microservices and monolithic architectures represent two fundamentally different approaches to software design, each with its own set of advantages and disadvantages. Hereâ€™s a detailed comparison:\r\n\r\n       Monolithic Architecture\r\n1.   Concept  :\r\n   - A monolithic architecture is a single, unified software application. All components and functions are tightly integrated and run as a single service.\r\n\r\n2.   Structure  :\r\n   - Typically consists of three main parts: a client-side user interface, a server-side application, and a database.\r\n   - All functionalities (e.g., business logic, data access, user interface) are combined into a single codebase and deployed together.\r\n\r\n3.   Advantages  :\r\n   -   Simplicity  : Easier to develop, test, and deploy because everything is in one place.\r\n   -   Performance  : Direct function calls within the same process can be faster than inter-service communication.\r\n   -   Development Speed  : Initially quicker to develop and deploy due to the single codebase and unified project structure.\r\n\r\n4.   Disadvantages  :\r\n   -   Scalability  : Harder to scale individual components independently. Scaling requires scaling the entire application.\r\n   -   Flexibility  : Changes in one part of the system can affect the entire application, making it difficult to adopt new technologies.\r\n   -   Maintenance  : Large codebase can become difficult to manage and understand over time.\r\n   -   Deployment  : Any change requires the whole application to be redeployed, increasing the risk and time of deployment.\r\n\r\n5.   Typical Use Cases  :\r\n   - Small to medium-sized applications.\r\n   - Applications with simple and stable requirements.\r\n   - Startups or projects in the initial development phase.\r\n\r\n       Microservices Architecture\r\n1.   Concept  :\r\n   - A microservices architecture divides the application into smaller, independent services that communicate over a network. Each service is responsible for a specific business function.\r\n\r\n2.   Structure  :\r\n   - Consists of multiple loosely coupled services, each with its own codebase, database, and deployment process.\r\n   - Services communicate with each other using lightweight protocols (e.g., HTTP/HTTPS, messaging queues).\r\n\r\n3.   Advantages  :\r\n   -   Scalability  : Individual services can be scaled independently based on demand.\r\n   -   Flexibility  : Easier to adopt new technologies and update services independently.\r\n   -   Resilience  : Failure in one service is less likely to bring down the entire system.\r\n   -   Deployment  : Independent deployment allows for more frequent and less risky updates.\r\n\r\n4.   Disadvantages  :\r\n   -   Complexity  : Increased complexity in managing multiple services, inter-service communication, and data consistency.\r\n   -   Operational Overhead  : Requires sophisticated infrastructure for deployment, monitoring, and managing services.\r\n   -   Performance  : Inter-service communication can introduce latency and requires careful management of network calls.\r\n\r\n5.   Typical Use Cases  :\r\n   - Large, complex applications with diverse and dynamic requirements.\r\n   - Applications requiring high scalability and flexibility.\r\n   - Organizations with DevOps practices and infrastructure to manage multiple services.\r\n\r\n       Key Differences\r\n-   Design Approach  :\r\n  -   Monolithic  : Single, unified application with all components tightly integrated.\r\n  -   Microservices  : Collection of small, independent services each responsible for a specific function.\r\n\r\n-   Scalability  :\r\n  -   Monolithic  : Harder to scale specific parts independently; entire application must be scaled.\r\n  -   Microservices  : Each service can be scaled independently based on demand.\r\n\r\n-   Development and Deployment  :\r\n  -   Monolithic  : Easier to develop and deploy initially but harder to manage and update as it grows.\r\n  -   Microservices  : More complex to develop and deploy initially but offers greater flexibility and easier management for large applications.\r\n\r\n-   Flexibility and Technology Stack  :\r\n  -   Monolithic  : Limited flexibility to adopt new technologies; changes impact the entire application.\r\n  -   Microservices  : Greater flexibility to use different technologies and update services independently.\r\n\r\n-   Fault Isolation  :\r\n  -   Monolithic  : Failure in one part can affect the entire application.\r\n  -   Microservices  : Failures are isolated to individual services, reducing the risk of a complete system failure.\r\n\r\n       Summary\r\nIn summary, monolithic architecture is simpler and faster to develop initially but can become unwieldy and harder to scale and maintain as the application grows. Microservices architecture offers greater flexibility, scalability, and fault isolation but comes with increased complexity and operational overhead. The choice between the two approaches depends on the specific needs and context of the application being developed.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 5,
        "topic": "system-design",
        "question": "What is difference between vertical and horizontal partitioning ?",
        "answer": "Vertical and horizontal partitioning are two techniques used in database design to improve performance, manageability, and scalability by dividing data across multiple tables or databases. Hereâ€™s a detailed comparison between the two:\r\n\r\n       Horizontal Partitioning (Sharding)\r\n1.   Concept  :\r\n   - Horizontal partitioning, also known as sharding, involves dividing a table's rows into multiple smaller, more manageable tables or shards. Each shard has the same schema but contains different rows of data.\r\n\r\n2.   Implementation  :\r\n   - Rows are distributed based on a shard key or partition key, such as a user ID or a geographic location.\r\n   - Each shard operates independently, often on separate physical servers or databases.\r\n\r\n3.   Advantages  :\r\n   -   Scalability  : Distributes data across multiple servers, improving performance and enabling horizontal scaling.\r\n   -   Load Distribution  : Spreads the load across multiple databases, reducing the risk of bottlenecks.\r\n   -   Fault Isolation  : Failures in one shard donâ€™t affect others, enhancing fault tolerance.\r\n\r\n4.   Disadvantages  :\r\n   -   Complexity  : Increased complexity in managing multiple databases and ensuring data consistency across shards.\r\n   -   Cross-shard Queries  : Queries that span multiple shards can be complex and less efficient.\r\n   -   Rebalancing  : Shard rebalancing (moving data between shards) can be challenging as data grows.\r\n\r\n5.   Typical Use Cases  :\r\n   - Large-scale applications with high data volume, such as social media platforms or e-commerce sites.\r\n   - Systems requiring high write and read throughput.\r\n\r\n       Vertical Partitioning\r\n1.   Concept  :\r\n   - Vertical partitioning involves dividing a tableâ€™s columns into multiple tables. Each table contains a subset of the original table's columns.\r\n\r\n2.   Implementation  :\r\n   - Columns are split based on access patterns, with frequently accessed columns placed together.\r\n   - Each partitioned table usually contains a primary key to allow rejoining if necessary.\r\n\r\n3.   Advantages  :\r\n   -   Performance  : Improves performance by reducing the amount of data read from disk for specific queries.\r\n   -   Manageability  : Simplifies database management by organizing related columns together.\r\n   -   Security  : Sensitive columns can be isolated, enhancing security.\r\n\r\n4.   Disadvantages  :\r\n   -   Joins  : Queries that require data from multiple partitions may need complex joins, which can be less efficient.\r\n   -   Redundancy  : Primary keys must be included in each partition, potentially leading to some redundancy.\r\n   -   Schema Evolution  : Changes to the schema can be more complex to manage.\r\n\r\n5.   Typical Use Cases  :\r\n   - Systems with wide tables where different columns are accessed by different queries.\r\n   - Databases where certain columns are accessed frequently while others are rarely used.\r\n\r\n       Key Differences\r\n-   Partitioning Basis  :\r\n  -   Horizontal Partitioning  : Divides data by rows, distributing different rows across multiple tables or databases.\r\n  -   Vertical Partitioning  : Divides data by columns, creating separate tables for different sets of columns.\r\n\r\n-   Scalability  :\r\n  -   Horizontal Partitioning  : Enables horizontal scaling by distributing load across multiple servers.\r\n  -   Vertical Partitioning  : Typically involves a single database, focusing on performance optimization rather than scaling.\r\n\r\n-   Complexity  :\r\n  -   Horizontal Partitioning  : More complex due to the need for managing multiple databases, sharding logic, and cross-shard operations.\r\n  -   Vertical Partitioning  : Simpler to implement but can lead to complex joins and schema management issues.\r\n\r\n-   Use Cases  :\r\n  -   Horizontal Partitioning  : Suitable for high-volume, distributed applications requiring significant scalability.\r\n  -   Vertical Partitioning  : Suitable for optimizing performance in systems with wide tables and varying access patterns.\r\n\r\n       Summary\r\nIn summary, horizontal partitioning (sharding) and vertical partitioning are two different strategies for dividing a database to improve performance and manageability. Horizontal partitioning splits data by rows across multiple databases or servers, enhancing scalability and fault tolerance but increasing complexity. Vertical partitioning splits data by columns within a single database, improving performance for specific queries and enhancing manageability but potentially complicating joins and schema management. The choice between these approaches depends on the specific requirements and characteristics of the application and its data access patterns.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 6,
        "topic": "system-design",
        "question": "What is Rate Limiter? How does it work?",
        "answer": "A rate limiter is a mechanism used to control the rate at which requests are processed in a system, such as an API, a web service, or a network. Its primary purpose is to ensure fair resource allocation, protect against abuse or overuse, and maintain service quality and stability. Here's a detailed explanation of what a rate limiter is and how it works:\r\n\r\n       Purpose of Rate Limiting\r\n1.   Preventing Abuse  : Protects the system from being overwhelmed by excessive requests, which can be caused by malicious users, automated scripts, or bugs.\r\n2.   Ensuring Fair Usage  : Ensures that no single user or client monopolizes resources, thereby providing a fair usage experience for all users.\r\n3.   Maintaining Performance  : Helps maintain optimal performance and availability of the service by preventing resource exhaustion.\r\n4.   Managing Costs  : Controls the usage of resources that might have associated costs, such as third-party API calls or server bandwidth.\r\n\r\n       How Rate Limiting Works\r\nRate limiting works by defining rules that limit the number of requests a client can make within a specified time period. Hereâ€™s an overview of the key components and mechanisms:\r\n\r\n1.   Rate Limit Policies  :\r\n   -   Fixed Window  : Limits requests within fixed time intervals. For example, 100 requests per minute.\r\n   -   Sliding Window  : Uses a rolling time window to limit requests. This approach smooths out bursts of traffic.\r\n   -   Token Bucket  : Tokens are added to a bucket at a fixed rate, and each request consumes a token. Requests are only allowed if there are sufficient tokens.\r\n   -   Leaky Bucket  : Similar to the token bucket, but requests are processed at a fixed rate, allowing bursts to be handled but smoothing them out over time.\r\n\r\n2.   Implementation Steps  :\r\n   -   Tracking Requests  : The system keeps track of the number of requests made by each client. This can be done using counters, timestamps, or tokens.\r\n   -   Evaluating Limits  : For each incoming request, the system checks if the client has exceeded their allowed rate.\r\n   -   Enforcing Limits  : If the limit is exceeded, the system responds with an error (commonly HTTP 429 Too Many Requests). Otherwise, the request is processed normally.\r\n   -   Resetting Counters  : Depending on the rate limit policy, counters or tokens are reset or refilled after the specified time period.\r\n\r\n3.   Implementation Techniques  :\r\n   -   In-Memory Storage  : For small-scale applications, rate limiting data can be stored in memory (e.g., using data structures like hash maps).\r\n   -   Distributed Storage  : For distributed systems, data can be stored in distributed caches like Redis or in databases to ensure consistency across multiple servers.\r\n   -   Middleware  : Rate limiting can be implemented as middleware in web servers or API gateways, intercepting requests before they reach the application logic.\r\n\r\n       Example Scenarios\r\n1.   API Rate Limiting  :\r\n   - An API might enforce a limit of 1,000 requests per hour per user to prevent a single user from overwhelming the server.\r\n   - Developers often receive rate limit quotas, such as 100 requests per minute, to ensure fair use of an API service.\r\n\r\n2.   Web Service Rate Limiting  :\r\n   - A website might limit login attempts to 5 per minute per IP address to prevent brute-force attacks.\r\n   - E-commerce sites might limit the number of product searches to 10 per second per user to prevent scraping.\r\n\r\n       Rate Limiting Strategies\r\n-   Global Rate Limiting  : Applies a rate limit across all users or clients uniformly.\r\n-   User-Specific Rate Limiting  : Applies rate limits based on individual user accounts or API keys.\r\n-   IP-Based Rate Limiting  : Limits requests based on the client's IP address.\r\n-   Geographic Rate Limiting  : Applies limits based on the geographical location of the request.\r\n\r\n       Example Implementation (Token Bucket Algorithm)\r\nHereâ€™s a simplified example of how the token bucket algorithm works:\r\n1.   Initialization  : A bucket is initialized with a maximum capacity of tokens (e.g., 100 tokens).\r\n2.   Token Refill  : Tokens are added to the bucket at a fixed rate (e.g., 1 token per second) up to the maximum capacity.\r\n3.   Request Handling  :\r\n   - When a request is received, the system checks if there are enough tokens in the bucket.\r\n   - If there are enough tokens, the corresponding number of tokens is removed, and the request is processed.\r\n   - If there are not enough tokens, the request is rejected, and an error is returned.\r\n\r\n       Summary\r\nIn summary, a rate limiter is a crucial tool for managing the rate of requests in a system to ensure fair use, prevent abuse, and maintain performance and stability. It works by defining rules and policies that limit the number of requests a client can make within a given timeframe and enforcing these limits using various algorithms and storage techniques. Rate limiting can be implemented at different levels, such as global, user-specific, or IP-based, depending on the application's needs.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 7,
        "topic": "system-design",
        "question": "How does Single Sign On (SSO) works?",
        "answer": "Single Sign-On (SSO) is an authentication process that allows a user to access multiple applications or systems with one set of login credentials (such as a username and password). Hereâ€™s an in-depth look at how SSO works, including the components involved and the steps in the process:\r\n\r\n       Components of SSO\r\n1.   User  : The individual who wants to access multiple services with one set of credentials.\r\n2.   Service Providers (SP)  : The applications or systems that the user wants to access.\r\n3.   Identity Provider (IdP)  : The centralized authentication service that validates the user's credentials and provides identity information to the service providers.\r\n4.   Authentication Token  : A secure token issued by the IdP after successful authentication, used to prove the user's identity to service providers.\r\n\r\n       How SSO Works\r\nHereâ€™s a step-by-step explanation of the SSO process:\r\n\r\n         Step 1: User Requests Access\r\n- The user tries to access a service provider (SP1).\r\n\r\n         Step 2: Redirect to Identity Provider\r\n- The service provider detects that the user is not authenticated and redirects the user to the Identity Provider (IdP) for authentication.\r\n\r\n         Step 3: User Authenticates with IdP\r\n- The user is prompted to log in at the IdP.\r\n- The user enters their credentials (username and password).\r\n- The IdP verifies the credentials.\r\n\r\n         Step 4: IdP Issues Authentication Token\r\n- Upon successful authentication, the IdP generates an authentication token.\r\n- The token contains information about the user (e.g., user ID, roles, permissions) and is usually signed to prevent tampering.\r\n\r\n         Step 5: Redirect Back to Service Provider\r\n- The IdP redirects the user back to the service provider (SP1), including the authentication token in the request.\r\n\r\n         Step 6: Service Provider Validates Token\r\n- The service provider receives the authentication token and validates it.\r\n- Validation typically involves checking the token's signature, expiry time, and ensuring it was issued by a trusted IdP.\r\n\r\n         Step 7: User Gains Access\r\n- Upon successful validation of the token, the service provider grants the user access to the requested resources.\r\n\r\n         Step 8: Accessing Additional Service Providers\r\n- When the user tries to access another service provider (SP2), the process is streamlined.\r\n- SP2 redirects the user to the IdP, but since the user is already authenticated, the IdP issues a new token without requiring the user to log in again.\r\n- The user is redirected back to SP2 with the new token, which is validated, and access is granted.\r\n\r\n       SSO Protocols and Standards\r\nSSO relies on various protocols and standards to securely manage authentication and token exchange. Some of the most common ones include:\r\n\r\n1.   SAML (Security Assertion Markup Language)  :\r\n   - An XML-based standard for exchanging authentication and authorization data between parties, particularly between an IdP and a service provider.\r\n   - Common in enterprise SSO implementations.\r\n\r\n2.   OAuth and OpenID Connect  :\r\n   - OAuth is an authorization framework that allows third-party applications to obtain limited access to a user's resources.\r\n   - OpenID Connect is an authentication layer built on top of OAuth 2.0, enabling SSO and identity management by providing an ID token.\r\n\r\n3.   Kerberos  :\r\n   - A network authentication protocol designed to provide strong authentication for client/server applications using secret-key cryptography.\r\n   - Commonly used in corporate environments for SSO within Windows domains.\r\n\r\n       Benefits of SSO\r\n-   User Convenience  : Users only need to remember one set of credentials, reducing password fatigue and improving the user experience.\r\n-   Improved Security  : Reduces the number of times users need to enter their credentials, lowering the risk of phishing attacks. Centralized authentication also enables stronger security policies.\r\n-   Administrative Efficiency  : Simplifies user management and reduces the administrative burden of handling multiple accounts.\r\n-   Consistency  : Provides a consistent authentication experience across multiple services and applications.\r\n\r\n       Challenges and Considerations\r\n-   Single Point of Failure  : If the IdP goes down, users may be unable to access any of the connected services.\r\n-   Complexity  : Implementing and maintaining an SSO system can be complex, especially in heterogeneous environments with various applications and platforms.\r\n-   Security Risks  : A compromise of the SSO credentials can potentially give an attacker access to all linked services. Strong security measures, such as multi-factor authentication (MFA), are crucial.\r\n\r\n       Example Scenario\r\nImagine an employee trying to access their companyâ€™s internal resources:\r\n1. The employee visits the companyâ€™s intranet (SP1).\r\n2. The intranet redirects the employee to the companyâ€™s SSO login page (IdP).\r\n3. The employee enters their credentials, and the IdP authenticates them.\r\n4. The IdP issues a token and redirects the employee back to the intranet.\r\n5. The intranet validates the token and grants access.\r\n6. Later, the employee wants to access the companyâ€™s email system (SP2). The email system redirects the employee to the IdP, which recognizes the authenticated session, issues a new token, and redirects back to the email system.\r\n7. The email system validates the token, and the employee gains access without re-entering credentials.\r\n\r\nIn summary, SSO simplifies the user experience by allowing access to multiple services with a single login, while also enhancing security and reducing administrative overhead.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 8,
        "topic": "system-design",
        "question": "How does Apache Kafka works? why it so fast?",
        "answer": "Apache Kafka is a distributed event streaming platform designed for high-throughput and low-latency data processing. It excels at handling real-time data feeds and integrates well with various data systems. Hereâ€™s a detailed explanation of how Kafka works and why it is so fast:\r\n\r\n       How Apache Kafka Works\r\n\r\n         Key Components\r\n1.   Producers  : Applications that send data to Kafka topics.\r\n2.   Consumers  : Applications that read data from Kafka topics.\r\n3.   Topics  : Categories or feed names to which messages are sent by producers. Each topic can be divided into multiple partitions.\r\n4.   Partitions  : Subdivisions of topics that enable parallel processing. Each partition is an ordered, immutable sequence of messages.\r\n5.   Brokers  : Kafka servers that store data and serve clients. Each broker hosts some partitions.\r\n6.   Clusters  : Groups of brokers working together. Kafka can scale horizontally by adding more brokers to the cluster.\r\n7.   Zookeeper  : A service that helps manage and coordinate Kafka brokers. It handles metadata about brokers, topics, and partitions.\r\n\r\n         Data Flow\r\n1.   Producer to Topic  : Producers publish messages to Kafka topics. Messages are distributed to different partitions within the topic, usually based on a key or in a round-robin fashion if no key is specified.\r\n2.   Replication  : Each partition has multiple replicas across different brokers to ensure data durability and high availability.\r\n3.   Consumer Groups  : Consumers are part of consumer groups. Each message in a partition is delivered to only one consumer in a group, enabling parallel processing and load balancing.\r\n4.   Offset Management  : Consumers keep track of their position (offset) within each partition, allowing them to resume reading from where they left off in case of a failure.\r\n\r\n       Why Apache Kafka is Fast\r\n\r\n         1.   Efficient Storage and Retrieval  \r\n-   Segmented Log Storage  : Kafka partitions are stored as logs segmented into files. New messages are appended to the end of these logs, making write operations sequential and extremely fast.\r\n-   Zero-Copy Transfer  : Kafka uses the zero-copy technique, leveraging the operating system's capability to directly transfer data from the page cache to the network socket without passing through user space. This reduces CPU and memory overhead.\r\n\r\n         2.   Horizontal Scalability  \r\n-   Partitioning  : Topics are divided into partitions, enabling parallelism. Multiple producers and consumers can work on different partitions simultaneously, ensuring scalability.\r\n-   Clustered Brokers  : Kafka clusters can be scaled horizontally by adding more brokers. Each broker can handle multiple partitions, spreading the load across the cluster.\r\n\r\n         3.   Efficient Data Handling  \r\n-   Batching and Compression  : Producers can batch messages together before sending them to Kafka, reducing the number of network calls. Kafka also supports message compression, reducing the amount of data transferred over the network.\r\n-   Memory-Mapped Files  : Kafka leverages memory-mapped files for reading and writing data, which allows the OS to manage file I/O efficiently and improves access speed.\r\n\r\n         4.   Optimized Network Usage  \r\n-   TCP Connections  : Kafka maintains persistent TCP connections between clients and brokers, reducing the overhead of connection setup and teardown.\r\n-   High Throughput  : Kafka can handle a large number of messages per second due to its efficient handling of network and disk I/O.\r\n\r\n         5.   Replication and Fault Tolerance  \r\n-   Leader-Follower Model  : Each partition has a designated leader and one or more followers. The leader handles all reads and writes while followers replicate the data. This replication ensures high availability and fault tolerance.\r\n-   Acknowledgement Mechanisms  : Kafka offers various levels of message durability, allowing producers to choose the required level of acknowledgment (e.g., acknowledgment from leader only or from all replicas).\r\n\r\n       Example Scenario\r\nConsider a real-time analytics platform where user activity data is collected and processed:\r\n1.   Producers  : User activity events (e.g., page views, clicks) are sent from web applications to Kafka topics.\r\n2.   Topics  : Data is categorized into topics like \"page_views\" and \"clicks\". Each topic has multiple partitions for parallel processing.\r\n3.   Consumers  : Analytics services consume the data from these topics to generate real-time insights and dashboards.\r\n4.   Scaling  : As user activity increases, more partitions and brokers are added to handle the higher load without impacting performance.\r\n\r\n       Summary\r\nApache Kafka's architecture and design principles make it exceptionally fast and scalable. Its efficient storage and retrieval mechanisms, horizontal scalability through partitioning and clustering, optimized data handling, and network usage, along with robust replication and fault tolerance, all contribute to its high performance. Kafka's ability to handle large volumes of data in real time makes it a preferred choice for modern data streaming applications.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 9,
        "topic": "system-design",
        "question": "Differnece between Kafka, ActiveMQ, and RabbitMQ",
        "answer": "Kafka, ActiveMQ, and RabbitMQ are all popular messaging systems, but they serve different purposes and have different architectures and use cases. Hereâ€™s a detailed comparison of the three:\r\n\r\n       1. Apache Kafka\r\n\r\n         Overview\r\n-   Type  : Distributed event streaming platform.\r\n-   Use Case  : High-throughput, low-latency real-time data streaming and event processing.\r\n-   Architecture  : Distributed, partitioned log system.\r\n\r\n         Key Features\r\n-   High Throughput  : Designed to handle large volumes of data with high throughput and low latency.\r\n-   Scalability  : Horizontally scalable with distributed partitions and replication.\r\n-   Durability  : Strong durability guarantees through log-based storage and replication.\r\n-   Replayability  : Messages can be replayed by consumers, enabling complex event processing and state reconstruction.\r\n-   Event Streaming  : Suited for real-time analytics, log aggregation, and stream processing.\r\n\r\n         Architecture and Mechanisms\r\n-   Storage  : Messages are stored in a distributed log, with each topic divided into partitions.\r\n-   Broker  : Kafka clusters consist of multiple brokers handling partitions.\r\n-   Consumer Model  : Consumer groups allow for distributed processing, where each message is processed by only one consumer in the group.\r\n-   Persistence  : Messages are persisted to disk and replicated across brokers for fault tolerance.\r\n\r\n       2. Apache ActiveMQ\r\n\r\n         Overview\r\n-   Type  : Traditional message broker with support for various messaging protocols.\r\n-   Use Case  : General-purpose messaging, enterprise application integration, and legacy system integration.\r\n-   Architecture  : Centralized broker-based architecture.\r\n\r\n         Key Features\r\n-   Protocol Support  : Supports multiple protocols including JMS, AMQP, MQTT, and STOMP.\r\n-   Complex Routing  : Advanced message routing capabilities, including topics, queues, and virtual destinations.\r\n-   Persistence Options  : Supports various persistence mechanisms, including in-memory, file-based, and database persistence.\r\n-   Transactions  : Supports JMS transactions, ensuring message delivery guarantees.\r\n\r\n         Architecture and Mechanisms\r\n-   Broker  : Centralized brokers manage message routing and delivery.\r\n-   Queue and Topic  : Supports both point-to-point (queue) and publish-subscribe (topic) messaging models.\r\n-   Durability  : Persistent storage options ensure messages are not lost.\r\n-   Delivery Modes  : Various delivery modes including at-most-once, at-least-once, and exactly-once.\r\n\r\n       3. RabbitMQ\r\n\r\n         Overview\r\n-   Type  : Message broker implementing the Advanced Message Queuing Protocol (AMQP).\r\n-   Use Case  : General-purpose messaging, real-time messaging, and background job processing.\r\n-   Architecture  : Centralized broker-based architecture.\r\n\r\n         Key Features\r\n-   AMQP Support  : Implements the AMQP protocol, providing robust message delivery guarantees and routing capabilities.\r\n-   Plugins  : Extensible with plugins for various features like monitoring, federation, and different protocols (e.g., MQTT, STOMP).\r\n-   Flexible Routing  : Complex routing capabilities with exchanges (direct, topic, fanout, headers) and bindings.\r\n-   High Availability  : Clustering and federation for high availability and load distribution.\r\n\r\n         Architecture and Mechanisms\r\n-   Broker  : Centralized brokers manage message routing and delivery.\r\n-   Exchange and Queue  : Messages are published to exchanges, which route them to queues based on binding rules.\r\n-   Durability  : Durable queues and messages ensure data is not lost.\r\n-   Delivery Acknowledgements  : Supports various acknowledgment modes to ensure reliable delivery.\r\n\r\n       Comparison Summary\r\n\r\n         1.   Use Cases  \r\n-   Kafka  : Ideal for real-time data streaming, log aggregation, event sourcing, and scalable data pipelines.\r\n-   ActiveMQ  : Suitable for enterprise application integration, legacy system integration, and scenarios requiring JMS compliance.\r\n-   RabbitMQ  : Great for real-time messaging, background job processing, and scenarios needing complex routing.\r\n\r\n         2.   Scalability  \r\n-   Kafka  : Highly scalable with a distributed architecture; handles large volumes of data with ease.\r\n-   ActiveMQ  : Can be scaled but typically used in smaller, more controlled environments.\r\n-   RabbitMQ  : Scalable with clustering and federation, but may require more configuration to achieve high scalability.\r\n\r\n         3.   Message Delivery  \r\n-   Kafka  : Strong durability and replayability with a log-based storage system.\r\n-   ActiveMQ  : Supports reliable delivery with persistent storage options and transactions.\r\n-   RabbitMQ  : Reliable delivery with durable queues and various acknowledgment modes.\r\n\r\n         4.   Architecture  \r\n-   Kafka  : Distributed, partitioned log system with a focus on throughput and durability.\r\n-   ActiveMQ  : Centralized broker-based system with rich protocol support and routing capabilities.\r\n-   RabbitMQ  : Centralized broker-based system with strong support for AMQP and flexible routing.\r\n\r\n       Choosing the Right Tool\r\n-   Kafka  : Choose Kafka for scenarios requiring high throughput, scalability, and real-time data processing.\r\n-   ActiveMQ  : Opt for ActiveMQ when you need compatibility with JMS or require support for a variety of messaging protocols.\r\n-   RabbitMQ  : Select RabbitMQ for reliable message delivery, complex routing, and ease of integration with various systems.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 10,
        "topic": "system-design",
        "question": "Difference between JWT, OAuth, and SAML?",
        "answer": "JWT (JSON Web Token), OAuth, and SAML (Security Assertion Markup Language) are all related to authentication and authorization, but they serve different purposes and operate in distinct ways. Hereâ€™s a detailed comparison of the three:\r\n\r\n       JWT (JSON Web Token)\r\n\r\n         Overview\r\n-   Type  : Token format\r\n-   Use Case  : Used for transmitting information securely between parties as a JSON object.\r\n\r\n         Key Features\r\n-   Structure  : Consists of three parts: Header, Payload, and Signature.\r\n  -   Header  : Contains the type of token (JWT) and the signing algorithm.\r\n  -   Payload  : Contains the claims, which are statements about an entity (typically the user) and additional data.\r\n  -   Signature  : Used to verify the token wasnâ€™t altered.\r\n-   Stateless  : JWTs are stateless, meaning the server does not store any session information. The token itself contains all the information needed.\r\n-   Compact and URL-safe  : Can be easily transmitted via URL, POST parameter, or inside an HTTP header.\r\n\r\n         Example Use Cases\r\n-   Authentication  : Once a user logs in, the server creates a JWT and sends it to the client. The client includes the JWT in subsequent requests to authenticated endpoints.\r\n-   Information Exchange  : Securely transmit information between parties.\r\n\r\n         How It Works\r\n1.   Creation  : The server generates a JWT and signs it with a secret or private key.\r\n2.   Transmission  : The JWT is sent to the client.\r\n3.   Verification  : The client includes the JWT in subsequent requests. The server verifies the tokenâ€™s signature to authenticate the request.\r\n\r\n       OAuth (Open Authorization)\r\n\r\n         Overview\r\n-   Type  : Authorization framework\r\n-   Use Case  : Allows third-party applications to access a userâ€™s resources without exposing their credentials.\r\n\r\n         Key Features\r\n-   Delegated Access  : Users can authorize third-party applications to perform actions on their behalf.\r\n-   Token-Based  : Uses access tokens to grant permissions.\r\n-   Flow Types  : Several types of flows (authorization code, implicit, resource owner password credentials, client credentials) to suit different scenarios.\r\n\r\n         Example Use Cases\r\n-   Third-Party Access  : Granting a third-party application access to a userâ€™s data on another service (e.g., allowing a social media app to access your contacts).\r\n\r\n         How It Works\r\n1.   Authorization Request  : The third-party application requests permission from the user to access resources.\r\n2.   Authorization Grant  : The user grants permission, and the application receives an authorization grant.\r\n3.   Token Exchange  : The application exchanges the authorization grant for an access token.\r\n4.   Access Resource  : The application uses the access token to access the user's resources.\r\n\r\n       SAML (Security Assertion Markup Language)\r\n\r\n         Overview\r\n-   Type  : Authentication and authorization protocol\r\n-   Use Case  : Primarily used for Single Sign-On (SSO) in enterprise environments.\r\n\r\n         Key Features\r\n-   XML-Based  : Uses XML for message format.\r\n-   SSO  : Enables Single Sign-On, allowing users to authenticate once and gain access to multiple applications.\r\n-   Federation  : Allows identity information to be shared across different security domains.\r\n\r\n         Example Use Cases\r\n-   Enterprise SSO  : Employees can log in once and access various internal applications without needing to log in separately to each one.\r\n\r\n         How It Works\r\n1.   User Request  : The user tries to access a service provider (SP).\r\n2.   Redirection to IdP  : The SP redirects the user to the identity provider (IdP) for authentication.\r\n3.   Authentication  : The user authenticates with the IdP.\r\n4.   SAML Assertion  : The IdP generates a SAML assertion (a token containing authentication information) and sends it to the SP.\r\n5.   Access Granted  : The SP validates the assertion and grants the user access.\r\n\r\n       Comparison Summary\r\n\r\n| Feature/Aspect    | JWT                         | OAuth                              | SAML                            |\r\n|-|--|||\r\n|   Type            | Token format                | Authorization framework            | Authentication and authorization protocol |\r\n|   Data Format     | JSON                        | Varies (often JSON or XML)         | XML                             |\r\n|   Primary Use     | Secure information exchange, stateless authentication | Delegated access to resources     | Single Sign-On (SSO)            |\r\n|   Structure       | Header, Payload, Signature  | Various flows and tokens           | Assertions, protocols, bindings |\r\n|   Transport       | HTTP headers, URL parameters| HTTP headers, URL parameters       | HTTP redirects, POST            |\r\n|   Stateless       | Yes                         | Yes                                | No (relies on server session state) |\r\n|   Token Expiry    | Included within token       | Managed by the authorization server| Defined by assertion validity    |\r\n|   Security        | Signature ensures integrity and authenticity | Relies on token integrity and scopes | Signature and encryption for assertions |\r\n|   Implementation  | Simple to implement and use | More complex, involves multiple steps | Typically more complex, involves configuration between IdP and SP |\r\n\r\n       Choosing the Right Tool\r\n-   JWT  : Best for stateless authentication and secure data exchange. Use when you need a simple, lightweight token to pass between services.\r\n-   OAuth  : Ideal for scenarios where third-party applications need to access user data without sharing credentials. Use for delegated access and when different flows are needed to suit various client types.\r\n-   SAML  : Best suited for enterprise environments where SSO is needed. Use when integrating with legacy systems and across different security domains.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 11,
        "topic": "system-design",
        "question": "What is System Design?",
        "answer": "System design refers to the process of defining the architecture, components, modules, interfaces, and data for a system to satisfy specified requirements. This process involves making a wide range of high-level decisions about the structure and behavior of the system. System design is a critical step in the development of complex systems and is essential for creating robust, scalable, and efficient solutions.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 12,
        "topic": "system-design",
        "question": "What is CAP theorem?",
        "answer": "CAP(Consistency-Availability-Partition Tolerance) theorem says that a distributed system cannot guarantee C, A and P simultaneously. It can at max provide any 2 of the 3 guarantees. Let us understand this with the help of a distributed database system. \n\n1.Consistency: This states that the data has to remain consistent after the execution of an operation in the database. For example, post database updation, all queries should retrieve the same result.\n2.Availability: The databases cannot have downtime and should be available and responsive always.\n3.Partition Tolerance: The database system should be functioning despite the communication becoming unstable.\nThe following image represents what databases guarantee what aspects of the CAP Theorem simultaneously. We see that RDBMS databases guarantee consistency and Availability simultaneously. Redis, MongoDB, Hbase databases guarantee Consistency and Partition Tolerance. Cassandra, CouchDB guarantees Availability and Partition Tolerance\nTrade-Offs According to CAP Theorem\nThe CAP theorem asserts that in the presence of a network partition, a distributed system can provide either consistency or availability, but not both. This results in three types of systems:\n\n1.CP (Consistency and Partition Tolerance):\n\nThese systems maintain consistency and can handle network partitions, but may sacrifice availability during a partition. An example is a traditional relational database system like MySQL when configured in a way to ensure strong consistency.\n\n2.AP (Availability and Partition Tolerance):\n\nThese systems are always available and can handle network partitions, but may return stale data or inconsistent results. An example is a NoSQL database like Cassandra, which is designed to be highly available and partition-tolerant.\n\n3.CA (Consistency and Availability):\n\nThese systems provide consistency and availability, but cannot handle network partitions. This combination is typically only possible in systems that are not distributed or in environments where network partitions are very unlikely.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 13,
        "topic": "system-design",
        "question": "What do you understand by load balancing? Why is it important in system design?",
        "answer": "Load balancing refers to the concept of distributing incoming traffic efficiently across a group of various backend servers. These servers are called server pools. Modern-day websites are designed to serve millions of requests from clients and return the responses in a fast and reliable manner. In order to serve these requests, the addition of more servers is required. In such a scenario, it is essential to distribute request traffic efficiently across each server so that they do not face undue loads. Load balancer acts as a traffic police cop facing the requests and routes them across the available servers in a way that not a single server is overwhelmed which could possibly degrade the application performance.\nWhen a server goes down, the load balancer redirects traffic to the remaining available servers. When a new server gets added to the configuration, the requests are automatically redirected to it. Following are the benefits of load balancers:\r\n\r\n1.They help to prevent requests from going to unhealthy or unavailable servers.\r\n2.Helps to prevent resources overloading.\r\n3.Helps to eliminate a single point of failure since the requests are routed to available servers whenever a server goes down.\r\n4.Requests sent to the servers are encrypted and the responses are decrypted. It aids in SSL termination and removes the need to install X.509 certificates on every server.\r\n5.Load balancing impacts system security and allows continuous software updates for accomodating changes in the system.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 14,
        "topic": "system-design",
        "question": "What do you understand by Latency, throughput, and availability of a system?",
        "answer": "Performance is an important factor in system design as it helps in making our services fast and reliable. Following are the three key metrics for measuring the performance:\r\n\r\n1.Latency: This is the time taken in milliseconds for delivering a single message.\r\n2.Throughput: This is the amount of data successfully transmitted through a system in a given amount of time. It is measured in bits per second.\r\n3.Availability: This determines the amount of time a system is available to respond to requests. It is calculated: System Uptime / (System Uptime+Downtime)",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 15,
        "topic": "system-design",
        "question": "What is Sharding?",
        "answer": "Sharding is a process of splitting the large logical dataset into multiple databases. It also refers to horizontal partitioning of data as it will be stored on multiple machines. By doing so, a sharded database becomes capable of handling more requests than a single large machine. Consider an example - in the following image, assume that we have around 1TB of data present in the database, when we perform sharding, we divide the large 1TB data into smaller chunks of 256GB into partitions called shards.\nSharding helps to scale databases by helping to handle the increased load by providing increased throughput, storage capacity and ensuring high availability.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 16,
        "topic": "system-design",
        "question": "How is NoSQL database different from SQL databases?",
        "answer": "SQL vs NoSQL Databases: Key Differences\nSQL (Structured Query Language) and NoSQL (Not Only SQL) databases are designed to store, retrieve, and manage data, but they differ significantly in their structure, scalability, and use cases.\n\n1. SQL (Relational) Databases\nSQL databases are structured, relational databases that store data in tables with predefined schemas.\n\nCharacteristics:\nStructured Data: Uses tables with rows and columns (like an Excel spreadsheet).\nPredefined Schema: A fixed schema must be defined before inserting data.\nUses SQL Queries: Standardized language for CRUD operations (SELECT, INSERT, UPDATE, DELETE).\nACID Compliance: Ensures data consistency with Atomicity, Consistency, Isolation, and Durability.\nVertical Scalability: Scaling is done by increasing CPU, RAM, or SSD on a single machine.\nExamples:\nMySQL\nPostgreSQL\nMicrosoft SQL Server\nOracle Database\nWhen to Use SQL?\nâœ” Data is structured and requires relationships (e.g., banking, finance, e-commerce).\nâœ” Transactions must be ACID-compliant (e.g., order processing, inventory management).\n\n2. NoSQL (Non-Relational) Databases\nNoSQL databases are designed for flexibility and scalability, handling large amounts of unstructured or semi-structured data.\n\nCharacteristics:\nSchema-less: No predefined structure; data can be stored as key-value pairs, documents, graphs, or wide-column stores.\nScalability: Supports horizontal scaling by adding more servers (distributed architecture).\nBASE Compliance: Focuses on Basic Availability, Soft-state, and Eventual consistency, which trades strong consistency for better performance.\nHandles Big Data: Designed for fast reads/writes in high-volume applications.\nTypes of NoSQL Databases:\nDocument Stores â†’ JSON-like structure (e.g., MongoDB, CouchDB).\nKey-Value Stores â†’ Simple key-value pairs (e.g., Redis, DynamoDB).\nWide-Column Stores â†’ Column-based storage (e.g., Cassandra, HBase).\nGraph Databases â†’ For complex relationships (e.g., Neo4j, ArangoDB).\nWhen to Use NoSQL?\nâœ” Flexible schema is needed (e.g., dynamic user profiles, social media).\nâœ” High-speed reads and writes are required (e.g., real-time analytics, caching).\nâœ” Data volume is massive and distributed (e.g., IoT, big data, logs).\n\nComparison Table: SQL vs NoSQL\nFeature                    SQL (Relational)                                                  NoSQL (Non-Relational)\nSchema        Fixed schema (tables, columns)                   Flexible schema (JSON, key-value, etc.)\nScalability        Vertical (scale-up: bigger machine)       Horizontal (scale-out: multiple machines)\nACID Compliance        Yes (strict consistency)                 No (eventual consistency)\nQuery Language        Uses SQL (structured queries)      NoSQL-specific (MongoDB Query, CQL, etc.)\nPerformance        Slower for large-scale reads/writes   Faster for high-volume, distributed data\nUse Cases        Banking, finance, CRM, ERP                        Social media, IoT, real-time apps\nWhich One to Choose?\nChoose SQL when data integrity, consistency, and structured relationships are important (e.g., financial systems, enterprise apps).\nChoose NoSQL when scalability, flexibility, and handling massive unstructured data are priorities (e.g., social networks, IoT, real-time apps).",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 17,
        "topic": "system-design",
        "question": "How is sharding different from partitioning?",
        "answer": "Database Sharding - Sharding is a technique for dividing a single dataset among many databases, allowing it to be stored across multiple workstations. Larger datasets can be divided into smaller parts and stored in numerous data nodes, boosting the systemâ€™s total storage capacity. A sharded database, similarly, can accommodate more requests than a single system by dividing the data over numerous machines. Sharding, also known as horizontal scaling or scale-out, is a type of scaling in which more nodes are added to distribute the load. Horizontal scaling provides near-limitless scalability for handling large amounts of data and high-volume tasks.\r\nDatabase Partitioning - Partitioning is the process of separating stored database objects (tables, indexes, and views) into distinct portions. Large database items are partitioned to improve controllability, performance, and availability. Partitioning can enhance performance when accessing partitioned tables in specific instances. Partitioning can act as a leading column in indexes, reducing index size and increasing the likelihood of finding the most desired indexes in memory. When a large portion of one area is used in the resultset, scanning that region is much faster than accessing data scattered throughout the entire table by index. Adding and deleting sections allows for large-scale data uploading and deletion, which improves performance. Data that are rarely used can be uploaded to more affordable data storage devices.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 18,
        "topic": "system-design",
        "question": "How is performance and scalability related to each other?",
        "answer": "A system is said to be scalable if there is increased performance is proportional to the resources added. Generally, performance increase in terms of scalability refers to serving more work units. But this can also mean being able to handle larger work units when datasets grow. If there is a performance problem in the application, then the system will be slow only for a single user. But if there is a scalability problem, then the system may be fast for a single user but it can get slow under heavy user load on the application.\nExamples\n\n1.Web Applications:\n\nA web application might perform well with a single server and a few users (good performance). As the number of users grows, the application can maintain its performance by adding more servers and distributing the load (good scalability).\n\n2.Databases:\n\nA database might provide fast query responses for a small dataset (good performance). To handle larger datasets and higher query volumes, it might need to shard data across multiple database instances (horizontal scalability).\n\n3.Microservices:\n\nA monolithic application might have excellent performance when deployed on a powerful server. However, as load increases, breaking it into microservices that can be independently scaled (scaling out each service as needed) can maintain or even improve overall system performance.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 19,
        "topic": "system-design",
        "question": "What is Caching? What are the various cache update strategies available in caching?",
        "answer": "Caching refers to the process of storing file copies in a temporary storage location called cache which helps in accessing data more quickly thereby reducing site latency. The cache can only store a limited amount of data. Due to this, it is important to determine cache update strategies that are best suited for the business requirements. Following are the various caching strategies available:\n\nCache-aside: In this strategy, our application is responsible to write and read data from the storage. Cache interaction with the storage is not direct. Here, the application looks for an entry in the cache, if the result is not found, then the entry is fetched from the database and is added to the cache for further use. Memcached is an example of using this update strategy.\nCache-aside strategy is also known as lazy loading because only the requested entry will be cached thereby avoiding unnecessary caching of the data. Some of the disadvantages of this strategy are:\n1.In cases of a cache miss, there would be a noticeable delay as it results in fetching data from the database and then caching it.\n2.The chances of data being stale are more if it is updated in the database. This can be reduced by defining the time-to-live parameter which forces an update of the cache entry.\n3.When a cache node fails, it will be replaced by a new, empty node which results in increased latency.\n\nWrite-through: In this strategy, the cache will be considered as the main data store by the system and the system reads and writes data into it. The cache then updates the database accordingly as shown in the database.\n1.The system adds or updates the entry in the cache.\n2.The cache synchronously writes entries to the database. This strategy is overall a slow operation because of the synchronous write operation. However, the 3.subsequent reads of the recently written data will be very fast. This strategy also ensures that the cache is not stale. But, there are chances that the data written in the cache might never be read. This issue can be reduced by providing appropriate TTL.\n\nWrite-behind (write-back): In this strategy, the application does the following steps:\n1.Add or update an entry in the cache\n2.Write the entry into the data store asynchronously for improving the write performance.\nThe main disadvantage of this method is that there are chances of data loss if the cache goes down before the contents of the cache are written into the database.\n\nRefresh-ahead: Using this strategy, we can configure the cache to refresh the cache entry automatically before its expiration.This cache strategy results in reduced latency if it can predict accurately what items are needed in future.\n\n",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 20,
        "topic": "system-design",
        "question": " What do you understand by Content delivery network?",
        "answer": "A Content Delivery Network (CDN) is a system of distributed servers that deliver web content and other digital assets to users based on their geographic location. The primary purpose of a CDN is to improve the performance, speed, and reliability of delivering content to end-users by minimizing latency, reducing load times, and enhancing overall user experience.\r\n\r\n       Key Components and Functions of a CDN:\r\n\r\n1.   Geographically Distributed Servers  :\r\n   -   Edge Servers  : Located in various locations (points of presence, or PoPs) closer to the end-users.\r\n   -   Origin Server  : The central server where the original content is hosted.\r\n\r\n2.   Caching  :\r\n   - CDN servers cache content, such as web pages, images, videos, stylesheets, and JavaScript files. This means that frequently accessed content is stored on multiple servers, reducing the need for repeated data retrieval from the origin server.\r\n\r\n3.   Load Balancing  :\r\n   - Distributes incoming traffic across multiple servers to prevent any single server from becoming overloaded. This ensures high availability and reliability.\r\n\r\n4.   Content Optimization  :\r\n   - CDNs often include features to optimize the content for faster delivery, such as compressing files, minifying code, and converting images to more efficient formats.\r\n\r\n5.   Security  :\r\n   - Provides various security features, including DDoS (Distributed Denial of Service) protection, web application firewalls (WAF), and secure sockets layer (SSL) certificates to ensure secure data transmission.\r\n\r\n6.   Analytics and Reporting  :\r\n   - Offers insights into traffic patterns, user behavior, and performance metrics to help website owners understand and optimize content delivery.\r\n\r\n       Benefits of Using a CDN:\r\n\r\n-   Reduced Latency  : By serving content from servers closer to the user, a CDN reduces the time it takes for data to travel, leading to faster load times.\r\n-   Improved Availability and Redundancy  : Distributing content across multiple servers ensures that even if one server goes down, others can take over, minimizing downtime.\r\n-   Scalability  : CDNs can handle large volumes of traffic and scale as demand increases without impacting performance.\r\n-   Bandwidth Savings  : By caching content and reducing the need to retrieve data from the origin server, CDNs help save bandwidth and reduce hosting costs.\r\n-   Enhanced Security  : CDNs provide robust security measures to protect against various cyber threats, ensuring safe content delivery.\r\n\r\n       Common Use Cases for CDNs:\r\n\r\n-   Websites and Web Applications  : Improving load times for websites, especially those with a global audience.\r\n-   Media and Streaming  : Delivering high-quality video and audio streams with minimal buffering.\r\n-   Software Distribution  : Efficiently distributing software updates and downloads to users worldwide.\r\n-   E-commerce  : Ensuring quick and reliable access to online stores, particularly during peak shopping periods.\r\n\r\nOverall, a CDN plays a crucial role in enhancing the performance, security, and scalability of delivering digital content to users across the globe.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 21,
        "topic": "system-design",
        "question": "What do you understand by Leader Election?",
        "answer": "Leader election is a process used in distributed computing to designate a single process (or node) as the organizer of some task distributed among several computers (nodes). This concept is crucial for ensuring coordination and consistency in distributed systems, such as databases, clusters, and networked systems, where multiple nodes must work together and agree on certain operations.\n\n       Key Concepts of Leader Election:\n\n1.   Consensus  : All nodes must agree on which node will act as the leader. This requires a consensus algorithm to ensure agreement despite potential faults and failures in the system.\n\n2.   Uniqueness  : At any given time, there should be exactly one leader. If there are multiple leaders, the system can become inconsistent and unreliable.\n\n3.   Fault Tolerance  : The leader election process must handle node failures gracefully. If the current leader fails, a new leader must be elected to maintain system operation.\n\n4.   Fairness  : All nodes should have an equal opportunity to be elected as the leader, preventing any node from being perpetually excluded from leadership.\n\n       Common Leader Election Algorithms:\n\n1.   Bully Algorithm  : \n   - Nodes send messages to each other to determine which has the highest ID. The node with the highest ID becomes the leader.\n   - If a node detects the current leader is down, it initiates a new election by sending messages to higher-ID nodes.\n   - The process continues until a single node remains that no other node can outvote.\n\n2.   Ring Algorithm  :\n   - Nodes are arranged in a logical ring.\n   - A node initiates an election by passing a token around the ring, containing its ID.\n   - Each node compares the token's ID with its own and passes the token with the higher ID.\n   - The process continues until the token returns to the initiator, who then knows the highest ID and announces the leader.\n\n3.   Raft Algorithm  :\n   - Part of the Raft consensus protocol, Raft ensures one of the nodes becomes the leader through a series of votes.\n   - Nodes start as followers, and if they don't hear from a leader, they become candidates.\n   - Candidates request votes from other nodes, and the node with a majority of votes becomes the leader.\n\n4.   Paxos Algorithm  :\n   - A more complex algorithm used for achieving consensus in a network of unreliable nodes.\n   - Nodes propose values, and a series of acceptor nodes vote on these proposals.\n   - If a proposal gets a majority, it is chosen, and the node that proposed it can act as the leader.\n\n       Applications of Leader Election:\n\n-   Distributed Databases  : Ensuring consistent access and updates to data by coordinating through a leader.\n-   Cluster Management  : Managing resources, tasks, and coordination in a cluster of servers or containers.\n-   Networking Protocols  : Managing routing decisions and data flow in a network.\n-   Fault-Tolerant Systems  : Ensuring system reliability by quickly electing new leaders if the current leader fails.\n\n       Challenges in Leader Election:\n\n-   Network Partitioning  : Ensuring consistency and availability when parts of the network become isolated.\n-   Node Failures  : Detecting and handling failures quickly to minimize disruption.\n-   Latency  : Minimizing the time taken to elect a new leader, especially in large distributed systems.\n\nLeader election is fundamental for the reliability and efficiency of distributed systems, enabling coordinated action and consistency among multiple independent nodes.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 22,
        "topic": "system-design",
        "question": "What are some of the design issues in distributed systems?",
        "answer": "Designing distributed systems involves addressing several complex issues to ensure that the system operates efficiently, reliably, and securely. Here are some of the key design issues in distributed systems:\r\n\r\n       1.   Scalability  \r\n-   Horizontal vs. Vertical Scaling  : Deciding whether to add more nodes (horizontal scaling) or enhance the capacity of existing nodes (vertical scaling).\r\n-   Load Balancing  : Distributing workload evenly across nodes to prevent any single node from becoming a bottleneck.\r\n-   Data Partitioning  : Splitting data across multiple nodes to improve access times and manageability.\r\n\r\n       2.   Fault Tolerance and Reliability  \r\n-   Redundancy  : Implementing data replication and redundant systems to ensure availability despite failures.\r\n-   Failure Detection  : Quickly and accurately detecting failures in nodes or network connections.\r\n-   Recovery Mechanisms  : Automatic failover procedures and data recovery processes to restore system operations after a failure.\r\n\r\n       3.   Consistency and Synchronization  \r\n-   Data Consistency Models  : Choosing the right consistency model (e.g., strong consistency, eventual consistency) to balance between performance and data accuracy.\r\n-   Concurrency Control  : Managing simultaneous operations on shared resources without conflicts, often using mechanisms like locks, timestamps, or versioning.\r\n-   Clock Synchronization  : Ensuring all nodes have a consistent notion of time, critical for time-sensitive operations and ordering events.\r\n\r\n       4.   Security  \r\n-   Authentication and Authorization  : Verifying identities and controlling access to resources.\r\n-   Data Encryption  : Protecting data in transit and at rest to prevent unauthorized access and tampering.\r\n-   Secure Communication  : Ensuring communication channels between nodes are secure to prevent eavesdropping and man-in-the-middle attacks.\r\n\r\n       5.   Network Issues  \r\n-   Latency  : Minimizing the delay between sending and receiving data across the network.\r\n-   Bandwidth  : Ensuring sufficient network capacity to handle the data load without causing delays or data loss.\r\n-   Partitioning  : Handling network partitions (where parts of the system become isolated) to maintain system operation and consistency.\r\n\r\n       6.   Resource Management  \r\n-   Efficient Resource Allocation  : Optimally allocating CPU, memory, storage, and network resources across nodes.\r\n-   Load Management  : Dynamically adjusting the allocation of resources based on current workload and performance metrics.\r\n-   Scalable Storage Solutions  : Implementing storage systems that can scale with increasing data volume and access demands.\r\n\r\n       7.   Heterogeneity  \r\n-   Interoperability  : Ensuring different hardware, operating systems, and software components can work together seamlessly.\r\n-   Standard Protocols  : Using standard communication protocols and data formats to enable diverse systems to interact.\r\n\r\n       8.   Transparency  \r\n-   Location Transparency  : Users and applications should not need to know the physical location of resources.\r\n-   Migration Transparency  : Resources can be moved or reallocated without affecting users or applications.\r\n-   Replication Transparency  : Multiple copies of resources should appear as a single resource to users.\r\n\r\n       9.   Coordination and Synchronization  \r\n-   Distributed Algorithms  : Implementing efficient algorithms for coordination tasks such as leader election, consensus, and mutual exclusion.\r\n-   Global State Management  : Keeping track of the overall state of the system in a consistent manner.\r\n\r\n       10.   Performance  \r\n-   Throughput and Latency Optimization  : Ensuring the system can handle high loads with minimal delay.\r\n-   Performance Monitoring  : Continuously monitoring system performance and identifying bottlenecks.\r\n-   Adaptive Systems  : Designing systems that can adapt to changing workloads and conditions dynamically.\r\n\r\n       11.   Data Management  \r\n-   Distributed Databases  : Ensuring data consistency, availability, and partition tolerance (CAP theorem).\r\n-   Replication Strategies  : Deciding how and where to replicate data to balance performance, reliability, and consistency.\r\n-   Query Processing and Optimization  : Efficiently processing queries in a distributed manner.\r\n\r\n       12.   Maintenance and Upgrades  \r\n-   Hot Swapping  : Making updates and upgrades without shutting down the system.\r\n-   Backward Compatibility  : Ensuring new versions of software can work with older versions without issues.\r\n\r\nAddressing these design issues effectively requires careful planning, robust architecture, and ongoing management to ensure that the distributed system remains efficient, reliable, and scalable.",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    },
    {
        "id": 23,
        "topic": "system-design",
        "question": "Stateful vs Stateless architecture ?",
        "answer": "Stateful vs Stateless Architecture\nStateful and stateless architectures define how a system manages data and user sessions. They play a crucial role in designing applications, APIs, and microservices.\n\n1. Stateful Architecture\nA stateful system maintains a session or state information about a user or process. Each request depends on previous interactions.\n\nCharacteristics:\nStores user session or state on the server.\nThe server retains information across multiple requests.\nRequires sticky sessions (requests must go to the same server).\nCan be more resource-intensive.\nExamples:\nTraditional Web Applications: A shopping cart on an e-commerce site where session data is stored on the server.\nDatabases: SQL databases like PostgreSQL, MySQL maintain state across transactions.\nFTP Servers: Maintain a connection between client and server.\nPros:\nâœ” Easier to maintain complex user sessions.\nâœ” Faster for connected users since session data is available.\n\nCons:\nâœ– Requires more memory and storage.\nâœ– Harder to scale because the state is stored on a single server.\n\n2. Stateless Architecture\nA stateless system does not store any session or state data. Each request is independent and must contain all the necessary information.\n\nCharacteristics:\nNo session is stored on the server.\nEvery request is treated independently.\nEasier to scale (load balancers can route requests to any server).\nRequires external storage if state needs to be retained (e.g., databases, caches).\nExamples:\nRESTful APIs: Each request contains all required data (e.g., authentication tokens).\nMicroservices: Stateless microservices can scale dynamically.\nCloud Functions (Serverless): AWS Lambda, Google Cloud Functions handle independent executions.\nPros:\nâœ” Scales easily with load balancers.\nâœ” Simpler, as no server-side session management is needed.\nâœ” Resilient, as failure of one instance does not affect others.\n\nCons:\nâœ– Requires external storage for session management (e.g., cookies, JWT tokens, databases).\nâœ– Can have higher latency since all data must be retrieved on every request.\n\n\nWhen to Use Which?\nUse Stateful when you need user sessions, real-time interactions, or persistent connections (e.g., banking apps, gaming servers).\nUse Stateless for scalable, distributed systems where requests should be independent (e.g., REST APIs, serverless computing).",
        "tags": [],
        "keyFeatures": [],
        "actionWords": [],
        "codeExample": ""
    }
]